{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deployed an optimized M2M100 model with ONNX"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optimize M2M100 model with ONNX\n",
    "\n",
    "\n",
    "In this notebook we will describes steps to load models with m2m100 translation models from HuggingFace and optimize them with ONNX Runtime. We will also show how to use the optimized model to perform translation.\n",
    "\n",
    "Once the model are optimize we will deploy them as an Api so that they can be used in a web application.\n",
    "\n",
    "At the first step we will load the vanilla model from Hugginface and use it for inference, then we will convert it to ONNX and Finally we will optimize it with ONNX Runtime."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First Step\n",
    "\n",
    "We will start by loading our model from the huggingface repository! \n",
    "\n",
    "Our model is an encoder decoder model from the m2m100 family. It was trained to translate english to swahili. Why did I pick Swahili? Because I am a native Swahili speaker. Let make sure we have the transformer library installed as well as Pytorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, M2M100ForConditionalGeneration, pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"masakhane/m2m100_418M_en_swa_rel_news\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model: M2M100ForConditionalGeneration = M2M100ForConditionalGeneration.from_pretrained(MODEL_NAME)\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_to_translate = \"Hello, my name is Espoir Murhabazi,  I am a Software Engineer from Congo DRC but living in UK\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_input = tokenizer(text_to_translate, return_tensors=\"pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_ids', 'attention_mask'])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_input.keys()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/esp.py/Projects/Personal/masakhane-web/.venv/lib/python3.11/site-packages/transformers/generation/utils.py:1353: UserWarning: Using `max_length`'s default (200) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "generated_tokens = model.generate(**model_input, forced_bos_token_id=tokenizer.lang_code_to_id[\"sw\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point our model have generate the translation token, the next step is to use our tokenizer to convert back the token to the text. This is called decoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "translated_text = tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Jina langu ni Espoir Murhabazi, Mimi ni mhandisi wa programu za kompyuta kutoka Kongo DRC lakini ninaishi Uingereza']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translated_text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The translated test show us that the model is working, the next step is to prepare the model for production. \n",
    "To productionarize our model we will deploy it to ONNX format.\n",
    "\n",
    "#### What is ONNX format?\n",
    "\n",
    "ONNX stands for Open Neural Network Exchange. It is an open format built to represent machine learning models.\n",
    "\n",
    "As you may know, neural networks are computation graphs with the input,  the weights  and operations.\n",
    "\n",
    "ONNX format is a way of saving neural network as computation graphs. That  computational graph represent the flow of data through the neural network.\n",
    "\n",
    "\n",
    "The keys benefits of saving neural networks in onnx format is interoperability and hardware access. A neural network saved in onnx format can be read by any deep learning platform.  A model trained in pytorch can be exported to ONNX format and then imported in Tensorflow and vice versa.\n",
    "\n",
    "You don't need to use python to read a model saved as ONNX, you can use any programming language of your choice such as javascript , c or c++. \n",
    "\n",
    "ONNX makes model easier to accesss hardware opitimizations, and you can apply other optimization such quantization to your ONNX model.\n",
    "\n",
    "Let us see how we can convert our model to ONNX format to use the full benefits of it."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trying to export the model manually and see if we can load the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To export the model to onnx format we will be using the optimum cli from Huggingface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The option --for-ort was passed, but its behavior is now the default in the ONNX exporter and passing it is not required anymore.\n",
      "Framework not specified. Using pt to export to ONNX.\n",
      "Using framework PyTorch: 2.0.0\n",
      "Overriding 1 configuration item(s)\n",
      "\t- use_cache -> False\n",
      "/Users/esp.py/Projects/Personal/masakhane-web/.venv/lib/python3.11/site-packages/transformers/models/m2m_100/modeling_m2m_100.py:174: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if max_pos > self.weights.size(0):\n",
      "/Users/esp.py/Projects/Personal/masakhane-web/.venv/lib/python3.11/site-packages/transformers/models/m2m_100/modeling_m2m_100.py:295: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if attn_weights.size() != (bsz * self.num_heads, tgt_len, src_len):\n",
      "/Users/esp.py/Projects/Personal/masakhane-web/.venv/lib/python3.11/site-packages/transformers/models/m2m_100/modeling_m2m_100.py:302: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if attention_mask.size() != (bsz, 1, tgt_len, src_len):\n",
      "/Users/esp.py/Projects/Personal/masakhane-web/.venv/lib/python3.11/site-packages/transformers/models/m2m_100/modeling_m2m_100.py:334: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if attn_output.size() != (bsz * self.num_heads, tgt_len, self.head_dim):\n",
      "================ Diagnostic Run torch.onnx.export version 2.0.0 ================\n",
      "verbose: False, log level: Level.ERROR\n",
      "======================= 0 NONE 0 NOTE 0 WARNING 0 ERROR ========================\n",
      "\n",
      "Using framework PyTorch: 2.0.0\n",
      "Overriding 1 configuration item(s)\n",
      "\t- use_cache -> True\n",
      "/Users/esp.py/Projects/Personal/masakhane-web/.venv/lib/python3.11/site-packages/transformers/models/m2m_100/modeling_m2m_100.py:1005: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if input_shape[-1] > 1:\n",
      "/Users/esp.py/Projects/Personal/masakhane-web/.venv/lib/python3.11/site-packages/transformers/models/m2m_100/modeling_m2m_100.py:83: TracerWarning: torch.tensor results are registered as constants in the trace. You can safely ignore this warning if you use this function to create tensors out of constant variables that would be the same every time you call this function. In any other case, this might cause the trace to be incorrect.\n",
      "  mask = torch.full((tgt_len, tgt_len), torch.tensor(torch.finfo(dtype).min, device=device), device=device)\n",
      "================ Diagnostic Run torch.onnx.export version 2.0.0 ================\n",
      "verbose: False, log level: Level.ERROR\n",
      "======================= 0 NONE 0 NOTE 0 WARNING 0 ERROR ========================\n",
      "\n",
      "Using framework PyTorch: 2.0.0\n",
      "Overriding 1 configuration item(s)\n",
      "\t- use_cache -> True\n",
      "Asked a sequence length of 16, but a sequence length of 1 will be used with use_past == True for `decoder_input_ids`.\n",
      "/Users/esp.py/Projects/Personal/masakhane-web/.venv/lib/python3.11/site-packages/transformers/models/m2m_100/modeling_m2m_100.py:257: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  and past_key_value[0].shape[2] == key_value_states.shape[1]\n",
      "================ Diagnostic Run torch.onnx.export version 2.0.0 ================\n",
      "verbose: False, log level: Level.ERROR\n",
      "======================= 0 NONE 0 NOTE 0 WARNING 0 ERROR ========================\n",
      "\n",
      "Post-processing the exported models...\n",
      "The two models proto have different outputs (49 and 25 outputs). Constant outputs will be added to unify the two models outputs.\n",
      "Addind a constant output for present.0.encoder.key of shape [0, 16, 1, 64] in model2.\n",
      "Addind a constant output for present.0.encoder.value of shape [0, 16, 1, 64] in model2.\n",
      "Addind a constant output for present.1.encoder.key of shape [0, 16, 1, 64] in model2.\n",
      "Addind a constant output for present.1.encoder.value of shape [0, 16, 1, 64] in model2.\n",
      "Addind a constant output for present.2.encoder.key of shape [0, 16, 1, 64] in model2.\n",
      "Addind a constant output for present.2.encoder.value of shape [0, 16, 1, 64] in model2.\n",
      "Addind a constant output for present.3.encoder.key of shape [0, 16, 1, 64] in model2.\n",
      "Addind a constant output for present.3.encoder.value of shape [0, 16, 1, 64] in model2.\n",
      "Addind a constant output for present.4.encoder.key of shape [0, 16, 1, 64] in model2.\n",
      "Addind a constant output for present.4.encoder.value of shape [0, 16, 1, 64] in model2.\n",
      "Addind a constant output for present.5.encoder.key of shape [0, 16, 1, 64] in model2.\n",
      "Addind a constant output for present.5.encoder.value of shape [0, 16, 1, 64] in model2.\n",
      "Addind a constant output for present.6.encoder.key of shape [0, 16, 1, 64] in model2.\n",
      "Addind a constant output for present.6.encoder.value of shape [0, 16, 1, 64] in model2.\n",
      "Addind a constant output for present.7.encoder.key of shape [0, 16, 1, 64] in model2.\n",
      "Addind a constant output for present.7.encoder.value of shape [0, 16, 1, 64] in model2.\n",
      "Addind a constant output for present.8.encoder.key of shape [0, 16, 1, 64] in model2.\n",
      "Addind a constant output for present.8.encoder.value of shape [0, 16, 1, 64] in model2.\n",
      "Addind a constant output for present.9.encoder.key of shape [0, 16, 1, 64] in model2.\n",
      "Addind a constant output for present.9.encoder.value of shape [0, 16, 1, 64] in model2.\n",
      "Addind a constant output for present.10.encoder.key of shape [0, 16, 1, 64] in model2.\n",
      "Addind a constant output for present.10.encoder.value of shape [0, 16, 1, 64] in model2.\n",
      "Addind a constant output for present.11.encoder.key of shape [0, 16, 1, 64] in model2.\n",
      "Addind a constant output for present.11.encoder.value of shape [0, 16, 1, 64] in model2.\n",
      "Validating models in subprocesses...\n",
      "Validating ONNX model onnx/m2m100_418M_en_swa_rel_news/encoder_model.onnx...\n",
      "\t-[✓] ONNX model output names match reference model (last_hidden_state)\n",
      "\t- Validating ONNX Model output \"last_hidden_state\":\n",
      "\t\t-[✓] (2, 16, 1024) matches (2, 16, 1024)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "Validating ONNX model onnx/m2m100_418M_en_swa_rel_news/decoder_model_merged.onnx...\n",
      "2023-10-12 22:39:12.829960 [W:onnxruntime:, graph.cc:3543 CleanUnusedInitializersAndNodeArgs] Removing initializer '/model/decoder/Shape_7_output_0'. It is not used by any node and should be removed from the model.\n",
      "2023-10-12 22:39:12.830010 [W:onnxruntime:, graph.cc:3543 CleanUnusedInitializersAndNodeArgs] Removing initializer '/model/decoder/Constant_22_output_0'. It is not used by any node and should be removed from the model.\n",
      "2023-10-12 22:39:12.830057 [W:onnxruntime:, graph.cc:3543 CleanUnusedInitializersAndNodeArgs] Removing initializer '/model/decoder/Constant_17_output_0'. It is not used by any node and should be removed from the model.\n",
      "2023-10-12 22:39:12.830066 [W:onnxruntime:, graph.cc:3543 CleanUnusedInitializersAndNodeArgs] Removing initializer '/model/decoder/Shape_4_output_0'. It is not used by any node and should be removed from the model.\n",
      "2023-10-12 22:39:12.830978 [W:onnxruntime:, graph.cc:3543 CleanUnusedInitializersAndNodeArgs] Removing initializer '/model/decoder/Shape_4_output_0'. It is not used by any node and should be removed from the model.\n",
      "2023-10-12 22:39:12.831852 [W:onnxruntime:, graph.cc:3543 CleanUnusedInitializersAndNodeArgs] Removing initializer '/model/decoder/Constant_8_output_0'. It is not used by any node and should be removed from the model.\n",
      "2023-10-12 22:39:12.880286 [W:onnxruntime:, graph.cc:3543 CleanUnusedInitializersAndNodeArgs] Removing initializer '/model/decoder/Constant_11_output_0'. It is not used by any node and should be removed from the model.\n",
      "2023-10-12 22:39:12.880322 [W:onnxruntime:, graph.cc:3543 CleanUnusedInitializersAndNodeArgs] Removing initializer '/model/decoder/Constant_13_output_0'. It is not used by any node and should be removed from the model.\n",
      "2023-10-12 22:39:12.880337 [W:onnxruntime:, graph.cc:3543 CleanUnusedInitializersAndNodeArgs] Removing initializer '/model/decoder/Constant_10_output_0'. It is not used by any node and should be removed from the model.\n",
      "2023-10-12 22:39:12.880343 [W:onnxruntime:, graph.cc:3543 CleanUnusedInitializersAndNodeArgs] Removing initializer '/model/decoder/Constant_12_output_0'. It is not used by any node and should be removed from the model.\n",
      "2023-10-12 22:39:12.880351 [W:onnxruntime:, graph.cc:3543 CleanUnusedInitializersAndNodeArgs] Removing initializer '/model/decoder/Constant_2_output_0'. It is not used by any node and should be removed from the model.\n",
      "2023-10-12 22:39:12.881086 [W:onnxruntime:, graph.cc:3543 CleanUnusedInitializersAndNodeArgs] Removing initializer '/model/decoder/Constant_1_output_0'. It is not used by any node and should be removed from the model.\n",
      "\t-[✓] ONNX model output names match reference model (present.5.encoder.key, present.8.decoder.value, present.6.encoder.value, present.0.decoder.value, present.9.encoder.value, present.0.decoder.key, present.5.decoder.value, present.11.decoder.key, present.5.encoder.value, present.9.decoder.value, present.11.encoder.key, present.1.encoder.value, present.6.decoder.key, present.3.encoder.key, present.0.encoder.key, present.1.decoder.key, present.10.decoder.value, present.6.encoder.key, present.3.decoder.value, present.3.encoder.value, present.8.decoder.key, present.1.decoder.value, present.7.decoder.value, present.2.decoder.key, present.2.encoder.value, present.4.decoder.value, present.10.encoder.value, present.3.decoder.key, present.8.encoder.value, present.5.decoder.key, present.7.encoder.value, logits, present.10.decoder.key, present.10.encoder.key, present.2.encoder.key, present.4.encoder.key, present.9.encoder.key, present.0.encoder.value, present.4.encoder.value, present.11.encoder.value, present.4.decoder.key, present.7.decoder.key, present.7.encoder.key, present.2.decoder.value, present.11.decoder.value, present.9.decoder.key, present.6.decoder.value, present.8.encoder.key, present.1.encoder.key)\n",
      "\t- Validating ONNX Model output \"logits\":\n",
      "\t\t-[✓] (2, 16, 128112) matches (2, 16, 128112)\n",
      "\t\t-[x] values not close enough, max diff: 1.4543533325195312e-05 (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.0.decoder.key\":\n",
      "\t\t-[✓] (2, 16, 16, 64) matches (2, 16, 16, 64)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.0.decoder.value\":\n",
      "\t\t-[✓] (2, 16, 16, 64) matches (2, 16, 16, 64)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.0.encoder.key\":\n",
      "\t\t-[✓] (2, 16, 16, 64) matches (2, 16, 16, 64)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.0.encoder.value\":\n",
      "\t\t-[✓] (2, 16, 16, 64) matches (2, 16, 16, 64)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.1.decoder.key\":\n",
      "\t\t-[✓] (2, 16, 16, 64) matches (2, 16, 16, 64)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.1.decoder.value\":\n",
      "\t\t-[✓] (2, 16, 16, 64) matches (2, 16, 16, 64)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.1.encoder.key\":\n",
      "\t\t-[✓] (2, 16, 16, 64) matches (2, 16, 16, 64)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.1.encoder.value\":\n",
      "\t\t-[✓] (2, 16, 16, 64) matches (2, 16, 16, 64)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.2.decoder.key\":\n",
      "\t\t-[✓] (2, 16, 16, 64) matches (2, 16, 16, 64)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.2.decoder.value\":\n",
      "\t\t-[✓] (2, 16, 16, 64) matches (2, 16, 16, 64)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.2.encoder.key\":\n",
      "\t\t-[✓] (2, 16, 16, 64) matches (2, 16, 16, 64)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.2.encoder.value\":\n",
      "\t\t-[✓] (2, 16, 16, 64) matches (2, 16, 16, 64)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.3.decoder.key\":\n",
      "\t\t-[✓] (2, 16, 16, 64) matches (2, 16, 16, 64)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.3.decoder.value\":\n",
      "\t\t-[✓] (2, 16, 16, 64) matches (2, 16, 16, 64)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.3.encoder.key\":\n",
      "\t\t-[✓] (2, 16, 16, 64) matches (2, 16, 16, 64)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.3.encoder.value\":\n",
      "\t\t-[✓] (2, 16, 16, 64) matches (2, 16, 16, 64)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.4.decoder.key\":\n",
      "\t\t-[✓] (2, 16, 16, 64) matches (2, 16, 16, 64)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.4.decoder.value\":\n",
      "\t\t-[✓] (2, 16, 16, 64) matches (2, 16, 16, 64)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.4.encoder.key\":\n",
      "\t\t-[✓] (2, 16, 16, 64) matches (2, 16, 16, 64)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.4.encoder.value\":\n",
      "\t\t-[✓] (2, 16, 16, 64) matches (2, 16, 16, 64)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.5.decoder.key\":\n",
      "\t\t-[✓] (2, 16, 16, 64) matches (2, 16, 16, 64)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.5.decoder.value\":\n",
      "\t\t-[✓] (2, 16, 16, 64) matches (2, 16, 16, 64)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.5.encoder.key\":\n",
      "\t\t-[✓] (2, 16, 16, 64) matches (2, 16, 16, 64)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.5.encoder.value\":\n",
      "\t\t-[✓] (2, 16, 16, 64) matches (2, 16, 16, 64)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.6.decoder.key\":\n",
      "\t\t-[✓] (2, 16, 16, 64) matches (2, 16, 16, 64)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.6.decoder.value\":\n",
      "\t\t-[✓] (2, 16, 16, 64) matches (2, 16, 16, 64)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.6.encoder.key\":\n",
      "\t\t-[✓] (2, 16, 16, 64) matches (2, 16, 16, 64)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.6.encoder.value\":\n",
      "\t\t-[✓] (2, 16, 16, 64) matches (2, 16, 16, 64)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.7.decoder.key\":\n",
      "\t\t-[✓] (2, 16, 16, 64) matches (2, 16, 16, 64)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.7.decoder.value\":\n",
      "\t\t-[✓] (2, 16, 16, 64) matches (2, 16, 16, 64)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.7.encoder.key\":\n",
      "\t\t-[✓] (2, 16, 16, 64) matches (2, 16, 16, 64)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.7.encoder.value\":\n",
      "\t\t-[✓] (2, 16, 16, 64) matches (2, 16, 16, 64)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.8.decoder.key\":\n",
      "\t\t-[✓] (2, 16, 16, 64) matches (2, 16, 16, 64)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.8.decoder.value\":\n",
      "\t\t-[✓] (2, 16, 16, 64) matches (2, 16, 16, 64)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.8.encoder.key\":\n",
      "\t\t-[✓] (2, 16, 16, 64) matches (2, 16, 16, 64)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.8.encoder.value\":\n",
      "\t\t-[✓] (2, 16, 16, 64) matches (2, 16, 16, 64)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.9.decoder.key\":\n",
      "\t\t-[✓] (2, 16, 16, 64) matches (2, 16, 16, 64)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.9.decoder.value\":\n",
      "\t\t-[✓] (2, 16, 16, 64) matches (2, 16, 16, 64)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.9.encoder.key\":\n",
      "\t\t-[✓] (2, 16, 16, 64) matches (2, 16, 16, 64)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.9.encoder.value\":\n",
      "\t\t-[✓] (2, 16, 16, 64) matches (2, 16, 16, 64)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.10.decoder.key\":\n",
      "\t\t-[✓] (2, 16, 16, 64) matches (2, 16, 16, 64)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.10.decoder.value\":\n",
      "\t\t-[✓] (2, 16, 16, 64) matches (2, 16, 16, 64)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.10.encoder.key\":\n",
      "\t\t-[✓] (2, 16, 16, 64) matches (2, 16, 16, 64)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.10.encoder.value\":\n",
      "\t\t-[✓] (2, 16, 16, 64) matches (2, 16, 16, 64)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.11.decoder.key\":\n",
      "\t\t-[✓] (2, 16, 16, 64) matches (2, 16, 16, 64)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.11.decoder.value\":\n",
      "\t\t-[✓] (2, 16, 16, 64) matches (2, 16, 16, 64)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.11.encoder.key\":\n",
      "\t\t-[✓] (2, 16, 16, 64) matches (2, 16, 16, 64)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.11.encoder.value\":\n",
      "\t\t-[✓] (2, 16, 16, 64) matches (2, 16, 16, 64)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "Validating ONNX model onnx/m2m100_418M_en_swa_rel_news/decoder_model_merged.onnx...\n",
      "Asked a sequence length of 16, but a sequence length of 1 will be used with use_past == True for `decoder_input_ids`.\n",
      "2023-10-12 22:39:30.650149 [W:onnxruntime:, graph.cc:3543 CleanUnusedInitializersAndNodeArgs] Removing initializer '/model/decoder/Shape_7_output_0'. It is not used by any node and should be removed from the model.\n",
      "2023-10-12 22:39:30.650204 [W:onnxruntime:, graph.cc:3543 CleanUnusedInitializersAndNodeArgs] Removing initializer '/model/decoder/Constant_22_output_0'. It is not used by any node and should be removed from the model.\n",
      "2023-10-12 22:39:30.650256 [W:onnxruntime:, graph.cc:3543 CleanUnusedInitializersAndNodeArgs] Removing initializer '/model/decoder/Constant_17_output_0'. It is not used by any node and should be removed from the model.\n",
      "2023-10-12 22:39:30.650267 [W:onnxruntime:, graph.cc:3543 CleanUnusedInitializersAndNodeArgs] Removing initializer '/model/decoder/Shape_4_output_0'. It is not used by any node and should be removed from the model.\n",
      "2023-10-12 22:39:30.651834 [W:onnxruntime:, graph.cc:3543 CleanUnusedInitializersAndNodeArgs] Removing initializer '/model/decoder/Shape_4_output_0'. It is not used by any node and should be removed from the model.\n",
      "2023-10-12 22:39:30.651909 [W:onnxruntime:, graph.cc:3543 CleanUnusedInitializersAndNodeArgs] Removing initializer '/model/decoder/Constant_8_output_0'. It is not used by any node and should be removed from the model.\n",
      "2023-10-12 22:39:30.711503 [W:onnxruntime:, graph.cc:3543 CleanUnusedInitializersAndNodeArgs] Removing initializer '/model/decoder/Constant_11_output_0'. It is not used by any node and should be removed from the model.\n",
      "2023-10-12 22:39:30.711532 [W:onnxruntime:, graph.cc:3543 CleanUnusedInitializersAndNodeArgs] Removing initializer '/model/decoder/Constant_13_output_0'. It is not used by any node and should be removed from the model.\n",
      "2023-10-12 22:39:30.711548 [W:onnxruntime:, graph.cc:3543 CleanUnusedInitializersAndNodeArgs] Removing initializer '/model/decoder/Constant_10_output_0'. It is not used by any node and should be removed from the model.\n",
      "2023-10-12 22:39:30.711553 [W:onnxruntime:, graph.cc:3543 CleanUnusedInitializersAndNodeArgs] Removing initializer '/model/decoder/Constant_12_output_0'. It is not used by any node and should be removed from the model.\n",
      "2023-10-12 22:39:30.711561 [W:onnxruntime:, graph.cc:3543 CleanUnusedInitializersAndNodeArgs] Removing initializer '/model/decoder/Constant_2_output_0'. It is not used by any node and should be removed from the model.\n",
      "2023-10-12 22:39:30.712362 [W:onnxruntime:, graph.cc:3543 CleanUnusedInitializersAndNodeArgs] Removing initializer '/model/decoder/Constant_1_output_0'. It is not used by any node and should be removed from the model.\n",
      "\t-[✓] ONNX model output names match reference model (present.4.decoder.value, present.7.decoder.value, present.11.decoder.value, present.9.decoder.key, present.3.decoder.value, present.2.decoder.value, present.2.decoder.key, present.0.decoder.key, present.3.decoder.key, present.8.decoder.value, present.5.decoder.value, present.6.decoder.key, present.4.decoder.key, logits, present.10.decoder.value, present.5.decoder.key, present.8.decoder.key, present.10.decoder.key, present.1.decoder.value, present.1.decoder.key, present.9.decoder.value, present.0.decoder.value, present.11.decoder.key, present.7.decoder.key, present.6.decoder.value)\n",
      "\t- Validating ONNX Model output \"logits\":\n",
      "\t\t-[✓] (2, 1, 128112) matches (2, 1, 128112)\n",
      "\t\t-[x] values not close enough, max diff: 2.09808349609375e-05 (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.0.decoder.key\":\n",
      "\t\t-[✓] (2, 16, 17, 64) matches (2, 16, 17, 64)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.0.decoder.value\":\n",
      "\t\t-[✓] (2, 16, 17, 64) matches (2, 16, 17, 64)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.1.decoder.key\":\n",
      "\t\t-[✓] (2, 16, 17, 64) matches (2, 16, 17, 64)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.1.decoder.value\":\n",
      "\t\t-[✓] (2, 16, 17, 64) matches (2, 16, 17, 64)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.2.decoder.key\":\n",
      "\t\t-[✓] (2, 16, 17, 64) matches (2, 16, 17, 64)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.2.decoder.value\":\n",
      "\t\t-[✓] (2, 16, 17, 64) matches (2, 16, 17, 64)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.3.decoder.key\":\n",
      "\t\t-[✓] (2, 16, 17, 64) matches (2, 16, 17, 64)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.3.decoder.value\":\n",
      "\t\t-[✓] (2, 16, 17, 64) matches (2, 16, 17, 64)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.4.decoder.key\":\n",
      "\t\t-[✓] (2, 16, 17, 64) matches (2, 16, 17, 64)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.4.decoder.value\":\n",
      "\t\t-[✓] (2, 16, 17, 64) matches (2, 16, 17, 64)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.5.decoder.key\":\n",
      "\t\t-[✓] (2, 16, 17, 64) matches (2, 16, 17, 64)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.5.decoder.value\":\n",
      "\t\t-[✓] (2, 16, 17, 64) matches (2, 16, 17, 64)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.6.decoder.key\":\n",
      "\t\t-[✓] (2, 16, 17, 64) matches (2, 16, 17, 64)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.6.decoder.value\":\n",
      "\t\t-[✓] (2, 16, 17, 64) matches (2, 16, 17, 64)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.7.decoder.key\":\n",
      "\t\t-[✓] (2, 16, 17, 64) matches (2, 16, 17, 64)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.7.decoder.value\":\n",
      "\t\t-[✓] (2, 16, 17, 64) matches (2, 16, 17, 64)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.8.decoder.key\":\n",
      "\t\t-[✓] (2, 16, 17, 64) matches (2, 16, 17, 64)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.8.decoder.value\":\n",
      "\t\t-[✓] (2, 16, 17, 64) matches (2, 16, 17, 64)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.9.decoder.key\":\n",
      "\t\t-[✓] (2, 16, 17, 64) matches (2, 16, 17, 64)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.9.decoder.value\":\n",
      "\t\t-[✓] (2, 16, 17, 64) matches (2, 16, 17, 64)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.10.decoder.key\":\n",
      "\t\t-[✓] (2, 16, 17, 64) matches (2, 16, 17, 64)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.10.decoder.value\":\n",
      "\t\t-[✓] (2, 16, 17, 64) matches (2, 16, 17, 64)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.11.decoder.key\":\n",
      "\t\t-[✓] (2, 16, 17, 64) matches (2, 16, 17, 64)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.11.decoder.value\":\n",
      "\t\t-[✓] (2, 16, 17, 64) matches (2, 16, 17, 64)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "Validation 0 for the model onnx/m2m100_418M_en_swa_rel_news/encoder_model.onnx raised: The maximum absolute difference between the output of the reference model and the ONNX exported model is not within the set tolerance 1e-05:\n",
      "- logits: max diff = 1.4543533325195312e-05\n",
      "The ONNX export succeeded with the warning: The maximum absolute difference between the output of the reference model and the ONNX exported model is not within the set tolerance 1e-05:\n",
      "- logits: max diff = 2.09808349609375e-05.\n",
      " The exported model was saved at: onnx/m2m100_418M_en_swa_rel_news\n"
     ]
    }
   ],
   "source": [
    "! optimum-cli export onnx --model masakhane/m2m100_418M_en_swa_rel_news --task seq2seq-lm-with-past --for-ort onnx/m2m100_418M_en_swa_rel_news\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "check if the model is correct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the previous command was run successfully, we can see our model saved at `onnx/m2m100_418M_en_swa_rel_news`. \n",
    "\n",
    "By checking the size we notice data our encoder model have 1.1 Gb, and our decoder model have 1.7Gb which make our model size to 2.8GB. Additionally, in the same folder we have the tokenizer data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model_onnx_dir = Path.cwd().joinpath('onnx').joinpath('m2m100_418M_en_swa_rel_news')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_model_onnx_dir.exists()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying Quantization"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quantization is the process of reducing the model size by using fewer bits to represent its parameters. Instead of using 32 bits precision floating points for most of the models, with quantization we can use 12 bits to represent a number and consequently reduce the size of the model.\n",
    "\n",
    "Smaller models resulting from quantization are faster to deploy and have low latency in production.\n",
    "For this tutorial we will use quantization to reduce the size of our model for inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from optimum.onnxruntime import ORTQuantizer, ORTModelForSeq2SeqLM\n",
    "from optimum.onnxruntime.configuration import AutoQuantizationConfig\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_quantizer = ORTQuantizer.from_pretrained(base_model_onnx_dir, file_name=\"encoder_model.onnx\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_quantizer = ORTQuantizer.from_pretrained(base_model_onnx_dir, file_name=\"decoder_model.onnx\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_with_past_quantizer = ORTQuantizer.from_pretrained(base_model_onnx_dir, file_name=\"decoder_with_past_model.onnx\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantizers = [encoder_quantizer, decoder_quantizer, decoder_with_past_quantizer]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use dynamic quantization to our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "dynamic_quantization_config = AutoQuantizationConfig.avx512_vnni(is_static=False, per_channel=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantized_model_path = Path(\"onnx\").joinpath(f\"{MODEL_SUFFIX}_quantized/\")\n",
    "quantized_model_path.mkdir(parents=True, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating dynamic quantizer: QOperator (mode: IntegerOps, schema: u8/s8, channel-wise: False)\n",
      "Quantizing model...\n",
      "Saving quantized model at: onnx/m2m100_418M_en_swa_rel_news_quantized (external data format: False)\n",
      "Configuration saved in onnx/m2m100_418M_en_swa_rel_news_quantized/ort_config.json\n",
      "Creating dynamic quantizer: QOperator (mode: IntegerOps, schema: u8/s8, channel-wise: False)\n",
      "Quantizing model...\n",
      "Saving quantized model at: onnx/m2m100_418M_en_swa_rel_news_quantized (external data format: False)\n",
      "Configuration saved in onnx/m2m100_418M_en_swa_rel_news_quantized/ort_config.json\n",
      "Creating dynamic quantizer: QOperator (mode: IntegerOps, schema: u8/s8, channel-wise: False)\n",
      "Quantizing model...\n",
      "Saving quantized model at: onnx/m2m100_418M_en_swa_rel_news_quantized (external data format: False)\n",
      "Configuration saved in onnx/m2m100_418M_en_swa_rel_news_quantized/ort_config.json\n"
     ]
    }
   ],
   "source": [
    "for quantizer in quantizers:\n",
    "    quantizer.quantize(quantization_config=dynamic_quantization_config, save_dir=quantized_model_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our model are save as quantized version, we can now check the size of the quantized models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the size of the model in MB is:  823.3566484451294\n",
      "the size of the model in MB is:  799.1366529464722\n",
      "the size of the model in MB is:  649.4426412582397\n"
     ]
    }
   ],
   "source": [
    "for model in quantized_model_path.glob(\"*.onnx\"):\n",
    "    print(\"the size of the model in MB is: \", model.stat().st_size / (1024 * 1024))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that we have managed to reduce the size of our initial models by two! From 1.6 Gb without quantization to 800 Mb with quantization. Let us see how to use the quantized model for inference."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use the quantized model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantized_model_path = base_model_onnx_dir\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-17 07:43:33.253277 [W:onnxruntime:, graph.cc:3543 CleanUnusedInitializersAndNodeArgs] Removing initializer '/model/decoder/Shape_7_output_0'. It is not used by any node and should be removed from the model.\n",
      "2023-10-17 07:43:33.253377 [W:onnxruntime:, graph.cc:3543 CleanUnusedInitializersAndNodeArgs] Removing initializer '/model/decoder/Constant_22_output_0'. It is not used by any node and should be removed from the model.\n",
      "2023-10-17 07:43:33.253444 [W:onnxruntime:, graph.cc:3543 CleanUnusedInitializersAndNodeArgs] Removing initializer '/model/decoder/Constant_17_output_0'. It is not used by any node and should be removed from the model.\n",
      "2023-10-17 07:43:33.254122 [W:onnxruntime:, graph.cc:3543 CleanUnusedInitializersAndNodeArgs] Removing initializer '/model/decoder/Shape_4_output_0'. It is not used by any node and should be removed from the model.\n",
      "2023-10-17 07:43:33.255608 [W:onnxruntime:, graph.cc:3543 CleanUnusedInitializersAndNodeArgs] Removing initializer '/model/decoder/Shape_4_output_0'. It is not used by any node and should be removed from the model.\n",
      "2023-10-17 07:43:33.255620 [W:onnxruntime:, graph.cc:3543 CleanUnusedInitializersAndNodeArgs] Removing initializer '/model/decoder/Constant_8_output_0'. It is not used by any node and should be removed from the model.\n",
      "2023-10-17 07:43:33.305738 [W:onnxruntime:, graph.cc:3543 CleanUnusedInitializersAndNodeArgs] Removing initializer '/model/decoder/Constant_11_output_0'. It is not used by any node and should be removed from the model.\n",
      "2023-10-17 07:43:33.305760 [W:onnxruntime:, graph.cc:3543 CleanUnusedInitializersAndNodeArgs] Removing initializer '/model/decoder/Constant_13_output_0'. It is not used by any node and should be removed from the model.\n",
      "2023-10-17 07:43:33.305774 [W:onnxruntime:, graph.cc:3543 CleanUnusedInitializersAndNodeArgs] Removing initializer '/model/decoder/Constant_10_output_0'. It is not used by any node and should be removed from the model.\n",
      "2023-10-17 07:43:33.305778 [W:onnxruntime:, graph.cc:3543 CleanUnusedInitializersAndNodeArgs] Removing initializer '/model/decoder/Constant_12_output_0'. It is not used by any node and should be removed from the model.\n",
      "2023-10-17 07:43:33.305785 [W:onnxruntime:, graph.cc:3543 CleanUnusedInitializersAndNodeArgs] Removing initializer '/model/decoder/Constant_2_output_0'. It is not used by any node and should be removed from the model.\n",
      "2023-10-17 07:43:33.307040 [W:onnxruntime:, graph.cc:3543 CleanUnusedInitializersAndNodeArgs] Removing initializer '/model/decoder/Constant_1_output_0'. It is not used by any node and should be removed from the model.\n",
      "2023-10-17 07:43:33.414387 [W:onnxruntime:, graph.cc:3543 CleanUnusedInitializersAndNodeArgs] Removing initializer '/model/decoder/layer_norm/Constant_output_0'. It is not used by any node and should be removed from the model.\n",
      "2023-10-17 07:43:33.414406 [W:onnxruntime:, graph.cc:3543 CleanUnusedInitializersAndNodeArgs] Removing initializer '/model/decoder/layers.11/final_layer_norm/Constant_1_output_0'. It is not used by any node and should be removed from the model.\n",
      "2023-10-17 07:43:33.414415 [W:onnxruntime:, graph.cc:3543 CleanUnusedInitializersAndNodeArgs] Removing initializer '/model/decoder/layers.11/encoder_attn_layer_norm/Constant_output_0'. It is not used by any node and should be removed from the model.\n",
      "2023-10-17 07:43:33.414422 [W:onnxruntime:, graph.cc:3543 CleanUnusedInitializersAndNodeArgs] Removing initializer '/model/decoder/layers.11/self_attn_layer_norm/Constant_1_output_0'. It is not used by any node and should be removed from the model.\n",
      "2023-10-17 07:43:33.414426 [W:onnxruntime:, graph.cc:3543 CleanUnusedInitializersAndNodeArgs] Removing initializer '/model/decoder/layers.10/final_layer_norm/Constant_1_output_0'. It is not used by any node and should be removed from the model.\n",
      "2023-10-17 07:43:33.414433 [W:onnxruntime:, graph.cc:3543 CleanUnusedInitializersAndNodeArgs] Removing initializer '/model/decoder/layers.10/encoder_attn_layer_norm/Constant_1_output_0'. It is not used by any node and should be removed from the model.\n",
      "2023-10-17 07:43:33.414440 [W:onnxruntime:, graph.cc:3543 CleanUnusedInitializersAndNodeArgs] Removing initializer '/model/decoder/layers.10/self_attn_layer_norm/Constant_1_output_0'. It is not used by any node and should be removed from the model.\n",
      "2023-10-17 07:43:33.414443 [W:onnxruntime:, graph.cc:3543 CleanUnusedInitializersAndNodeArgs] Removing initializer '/model/decoder/layers.10/self_attn_layer_norm/Constant_output_0'. It is not used by any node and should be removed from the model.\n",
      "2023-10-17 07:43:33.414447 [W:onnxruntime:, graph.cc:3543 CleanUnusedInitializersAndNodeArgs] Removing initializer '/model/decoder/layers.9/final_layer_norm/Constant_1_output_0'. It is not used by any node and should be removed from the model.\n",
      "2023-10-17 07:43:33.414537 [W:onnxruntime:, graph.cc:3543 CleanUnusedInitializersAndNodeArgs] Removing initializer '/model/decoder/layers.9/final_layer_norm/Constant_output_0'. It is not used by any node and should be removed from the model.\n",
      "2023-10-17 07:43:33.414554 [W:onnxruntime:, graph.cc:3543 CleanUnusedInitializersAndNodeArgs] Removing initializer '/model/decoder/layers.9/encoder_attn_layer_norm/Constant_1_output_0'. It is not used by any node and should be removed from the model.\n",
      "2023-10-17 07:43:33.414563 [W:onnxruntime:, graph.cc:3543 CleanUnusedInitializersAndNodeArgs] Removing initializer '/model/decoder/layers.9/self_attn_layer_norm/Constant_1_output_0'. It is not used by any node and should be removed from the model.\n",
      "2023-10-17 07:43:33.414567 [W:onnxruntime:, graph.cc:3543 CleanUnusedInitializersAndNodeArgs] Removing initializer '/model/decoder/layers.9/self_attn_layer_norm/Constant_output_0'. It is not used by any node and should be removed from the model.\n",
      "2023-10-17 07:43:33.414571 [W:onnxruntime:, graph.cc:3543 CleanUnusedInitializersAndNodeArgs] Removing initializer '/model/decoder/layers.8/final_layer_norm/Constant_1_output_0'. It is not used by any node and should be removed from the model.\n",
      "2023-10-17 07:43:33.414575 [W:onnxruntime:, graph.cc:3543 CleanUnusedInitializersAndNodeArgs] Removing initializer '/model/decoder/layers.8/encoder_attn_layer_norm/Constant_1_output_0'. It is not used by any node and should be removed from the model.\n",
      "2023-10-17 07:43:33.414579 [W:onnxruntime:, graph.cc:3543 CleanUnusedInitializersAndNodeArgs] Removing initializer '/model/decoder/layers.8/encoder_attn_layer_norm/Constant_output_0'. It is not used by any node and should be removed from the model.\n",
      "2023-10-17 07:43:33.414585 [W:onnxruntime:, graph.cc:3543 CleanUnusedInitializersAndNodeArgs] Removing initializer '/model/decoder/layers.7/final_layer_norm/Constant_1_output_0'. It is not used by any node and should be removed from the model.\n",
      "2023-10-17 07:43:33.414589 [W:onnxruntime:, graph.cc:3543 CleanUnusedInitializersAndNodeArgs] Removing initializer '/model/decoder/layers.7/final_layer_norm/Constant_output_0'. It is not used by any node and should be removed from the model.\n",
      "2023-10-17 07:43:33.414596 [W:onnxruntime:, graph.cc:3543 CleanUnusedInitializersAndNodeArgs] Removing initializer '/model/decoder/layers.8/self_attn_layer_norm/Constant_output_0'. It is not used by any node and should be removed from the model.\n",
      "2023-10-17 07:43:33.414602 [W:onnxruntime:, graph.cc:3543 CleanUnusedInitializersAndNodeArgs] Removing initializer '/model/decoder/layers.7/self_attn_layer_norm/Constant_1_output_0'. It is not used by any node and should be removed from the model.\n",
      "2023-10-17 07:43:33.414606 [W:onnxruntime:, graph.cc:3543 CleanUnusedInitializersAndNodeArgs] Removing initializer '/model/decoder/layers.7/self_attn_layer_norm/Constant_output_0'. It is not used by any node and should be removed from the model.\n",
      "2023-10-17 07:43:33.414609 [W:onnxruntime:, graph.cc:3543 CleanUnusedInitializersAndNodeArgs] Removing initializer '/model/decoder/layers.6/final_layer_norm/Constant_output_0'. It is not used by any node and should be removed from the model.\n",
      "2023-10-17 07:43:33.414614 [W:onnxruntime:, graph.cc:3543 CleanUnusedInitializersAndNodeArgs] Removing initializer '/model/decoder/layer_norm/Constant_1_output_0'. It is not used by any node and should be removed from the model.\n",
      "2023-10-17 07:43:33.414619 [W:onnxruntime:, graph.cc:3543 CleanUnusedInitializersAndNodeArgs] Removing initializer '/model/decoder/layers.6/encoder_attn_layer_norm/Constant_output_0'. It is not used by any node and should be removed from the model.\n",
      "2023-10-17 07:43:33.414626 [W:onnxruntime:, graph.cc:3543 CleanUnusedInitializersAndNodeArgs] Removing initializer '/model/decoder/layers.5/final_layer_norm/Constant_1_output_0'. It is not used by any node and should be removed from the model.\n",
      "2023-10-17 07:43:33.414633 [W:onnxruntime:, graph.cc:3543 CleanUnusedInitializersAndNodeArgs] Removing initializer '/model/decoder/layers.7/encoder_attn_layer_norm/Constant_1_output_0'. It is not used by any node and should be removed from the model.\n",
      "2023-10-17 07:43:33.414637 [W:onnxruntime:, graph.cc:3543 CleanUnusedInitializersAndNodeArgs] Removing initializer '/model/decoder/layers.5/encoder_attn_layer_norm/Constant_1_output_0'. It is not used by any node and should be removed from the model.\n",
      "2023-10-17 07:43:33.414641 [W:onnxruntime:, graph.cc:3543 CleanUnusedInitializersAndNodeArgs] Removing initializer '/model/decoder/layers.5/encoder_attn_layer_norm/Constant_output_0'. It is not used by any node and should be removed from the model.\n",
      "2023-10-17 07:43:33.414645 [W:onnxruntime:, graph.cc:3543 CleanUnusedInitializersAndNodeArgs] Removing initializer '/model/decoder/layers.0/encoder_attn_layer_norm/Constant_1_output_0'. It is not used by any node and should be removed from the model.\n",
      "2023-10-17 07:43:33.414651 [W:onnxruntime:, graph.cc:3543 CleanUnusedInitializersAndNodeArgs] Removing initializer '/model/decoder/layers.5/self_attn_layer_norm/Constant_1_output_0'. It is not used by any node and should be removed from the model.\n",
      "2023-10-17 07:43:33.414654 [W:onnxruntime:, graph.cc:3543 CleanUnusedInitializersAndNodeArgs] Removing initializer '/model/decoder/layers.5/self_attn_layer_norm/Constant_output_0'. It is not used by any node and should be removed from the model.\n",
      "2023-10-17 07:43:33.414658 [W:onnxruntime:, graph.cc:3543 CleanUnusedInitializersAndNodeArgs] Removing initializer '/model/decoder/layers.4/final_layer_norm/Constant_1_output_0'. It is not used by any node and should be removed from the model.\n",
      "2023-10-17 07:43:33.414662 [W:onnxruntime:, graph.cc:3543 CleanUnusedInitializersAndNodeArgs] Removing initializer '/model/decoder/layers.4/final_layer_norm/Constant_output_0'. It is not used by any node and should be removed from the model.\n",
      "2023-10-17 07:43:33.414667 [W:onnxruntime:, graph.cc:3543 CleanUnusedInitializersAndNodeArgs] Removing initializer '/model/decoder/layers.9/encoder_attn_layer_norm/Constant_output_0'. It is not used by any node and should be removed from the model.\n",
      "2023-10-17 07:43:33.414673 [W:onnxruntime:, graph.cc:3543 CleanUnusedInitializersAndNodeArgs] Removing initializer '/model/decoder/layers.6/self_attn_layer_norm/Constant_output_0'. It is not used by any node and should be removed from the model.\n",
      "2023-10-17 07:43:33.414681 [W:onnxruntime:, graph.cc:3543 CleanUnusedInitializersAndNodeArgs] Removing initializer '/model/decoder/layers.4/self_attn_layer_norm/Constant_output_0'. It is not used by any node and should be removed from the model.\n",
      "2023-10-17 07:43:33.414685 [W:onnxruntime:, graph.cc:3543 CleanUnusedInitializersAndNodeArgs] Removing initializer '/model/decoder/layers.3/final_layer_norm/Constant_1_output_0'. It is not used by any node and should be removed from the model.\n",
      "2023-10-17 07:43:33.414695 [W:onnxruntime:, graph.cc:3543 CleanUnusedInitializersAndNodeArgs] Removing initializer '/model/decoder/layers.6/final_layer_norm/Constant_1_output_0'. It is not used by any node and should be removed from the model.\n",
      "2023-10-17 07:43:33.414701 [W:onnxruntime:, graph.cc:3543 CleanUnusedInitializersAndNodeArgs] Removing initializer '/model/decoder/layers.3/encoder_attn_layer_norm/Constant_output_0'. It is not used by any node and should be removed from the model.\n",
      "2023-10-17 07:43:33.414705 [W:onnxruntime:, graph.cc:3543 CleanUnusedInitializersAndNodeArgs] Removing initializer '/model/decoder/layers.4/encoder_attn_layer_norm/Constant_1_output_0'. It is not used by any node and should be removed from the model.\n",
      "2023-10-17 07:43:33.414710 [W:onnxruntime:, graph.cc:3543 CleanUnusedInitializersAndNodeArgs] Removing initializer '/model/decoder/layers.3/self_attn_layer_norm/Constant_1_output_0'. It is not used by any node and should be removed from the model.\n",
      "2023-10-17 07:43:33.414714 [W:onnxruntime:, graph.cc:3543 CleanUnusedInitializersAndNodeArgs] Removing initializer '/model/decoder/layers.3/self_attn_layer_norm/Constant_output_0'. It is not used by any node and should be removed from the model.\n",
      "2023-10-17 07:43:33.414718 [W:onnxruntime:, graph.cc:3543 CleanUnusedInitializersAndNodeArgs] Removing initializer '/model/decoder/layers.6/encoder_attn_layer_norm/Constant_1_output_0'. It is not used by any node and should be removed from the model.\n",
      "2023-10-17 07:43:33.414728 [W:onnxruntime:, graph.cc:3543 CleanUnusedInitializersAndNodeArgs] Removing initializer '/model/decoder/layers.3/encoder_attn_layer_norm/Constant_1_output_0'. It is not used by any node and should be removed from the model.\n",
      "2023-10-17 07:43:33.414733 [W:onnxruntime:, graph.cc:3543 CleanUnusedInitializersAndNodeArgs] Removing initializer '/model/decoder/layers.2/encoder_attn_layer_norm/Constant_1_output_0'. It is not used by any node and should be removed from the model.\n",
      "2023-10-17 07:43:33.414739 [W:onnxruntime:, graph.cc:3543 CleanUnusedInitializersAndNodeArgs] Removing initializer '/model/decoder/layers.2/final_layer_norm/Constant_1_output_0'. It is not used by any node and should be removed from the model.\n",
      "2023-10-17 07:43:33.414743 [W:onnxruntime:, graph.cc:3543 CleanUnusedInitializersAndNodeArgs] Removing initializer '/model/decoder/layers.1/encoder_attn_layer_norm/Constant_1_output_0'. It is not used by any node and should be removed from the model.\n",
      "2023-10-17 07:43:33.414748 [W:onnxruntime:, graph.cc:3543 CleanUnusedInitializersAndNodeArgs] Removing initializer '/model/decoder/layers.2/final_layer_norm/Constant_output_0'. It is not used by any node and should be removed from the model.\n",
      "2023-10-17 07:43:33.414752 [W:onnxruntime:, graph.cc:3543 CleanUnusedInitializersAndNodeArgs] Removing initializer '/model/decoder/layers.2/self_attn_layer_norm/Constant_1_output_0'. It is not used by any node and should be removed from the model.\n",
      "2023-10-17 07:43:33.414756 [W:onnxruntime:, graph.cc:3543 CleanUnusedInitializersAndNodeArgs] Removing initializer '/model/decoder/layers.1/final_layer_norm/Constant_1_output_0'. It is not used by any node and should be removed from the model.\n",
      "2023-10-17 07:43:33.414760 [W:onnxruntime:, graph.cc:3543 CleanUnusedInitializersAndNodeArgs] Removing initializer '/model/decoder/layers.1/final_layer_norm/Constant_output_0'. It is not used by any node and should be removed from the model.\n",
      "2023-10-17 07:43:33.414765 [W:onnxruntime:, graph.cc:3543 CleanUnusedInitializersAndNodeArgs] Removing initializer '/model/decoder/layers.8/final_layer_norm/Constant_output_0'. It is not used by any node and should be removed from the model.\n",
      "2023-10-17 07:43:33.414770 [W:onnxruntime:, graph.cc:3543 CleanUnusedInitializersAndNodeArgs] Removing initializer '/model/decoder/layers.11/final_layer_norm/Constant_output_0'. It is not used by any node and should be removed from the model.\n",
      "2023-10-17 07:43:33.414776 [W:onnxruntime:, graph.cc:3543 CleanUnusedInitializersAndNodeArgs] Removing initializer '/model/decoder/layers.7/encoder_attn_layer_norm/Constant_output_0'. It is not used by any node and should be removed from the model.\n",
      "2023-10-17 07:43:33.414781 [W:onnxruntime:, graph.cc:3543 CleanUnusedInitializersAndNodeArgs] Removing initializer '/model/decoder/layers.1/encoder_attn_layer_norm/Constant_output_0'. It is not used by any node and should be removed from the model.\n",
      "2023-10-17 07:43:33.414788 [W:onnxruntime:, graph.cc:3543 CleanUnusedInitializersAndNodeArgs] Removing initializer '/model/decoder/layers.0/final_layer_norm/Constant_output_0'. It is not used by any node and should be removed from the model.\n",
      "2023-10-17 07:43:33.414793 [W:onnxruntime:, graph.cc:3543 CleanUnusedInitializersAndNodeArgs] Removing initializer '/model/decoder/layers.10/encoder_attn_layer_norm/Constant_output_0'. It is not used by any node and should be removed from the model.\n",
      "2023-10-17 07:43:33.414796 [W:onnxruntime:, graph.cc:3543 CleanUnusedInitializersAndNodeArgs] Removing initializer '/model/decoder/layers.0/self_attn_layer_norm/Constant_1_output_0'. It is not used by any node and should be removed from the model.\n",
      "2023-10-17 07:43:33.414800 [W:onnxruntime:, graph.cc:3543 CleanUnusedInitializersAndNodeArgs] Removing initializer '/model/decoder/layers.5/final_layer_norm/Constant_output_0'. It is not used by any node and should be removed from the model.\n",
      "2023-10-17 07:43:33.414805 [W:onnxruntime:, graph.cc:3543 CleanUnusedInitializersAndNodeArgs] Removing initializer '/model/decoder/layers.1/self_attn_layer_norm/Constant_output_0'. It is not used by any node and should be removed from the model.\n",
      "2023-10-17 07:43:33.414808 [W:onnxruntime:, graph.cc:3543 CleanUnusedInitializersAndNodeArgs] Removing initializer '/model/decoder/layers.4/encoder_attn_layer_norm/Constant_output_0'. It is not used by any node and should be removed from the model.\n",
      "2023-10-17 07:43:33.414813 [W:onnxruntime:, graph.cc:3543 CleanUnusedInitializersAndNodeArgs] Removing initializer '/model/decoder/layers.11/self_attn_layer_norm/Constant_output_0'. It is not used by any node and should be removed from the model.\n",
      "2023-10-17 07:43:33.414817 [W:onnxruntime:, graph.cc:3543 CleanUnusedInitializersAndNodeArgs] Removing initializer '/model/decoder/layers.6/self_attn_layer_norm/Constant_1_output_0'. It is not used by any node and should be removed from the model.\n",
      "2023-10-17 07:43:33.414821 [W:onnxruntime:, graph.cc:3543 CleanUnusedInitializersAndNodeArgs] Removing initializer '/model/decoder/layers.0/final_layer_norm/Constant_1_output_0'. It is not used by any node and should be removed from the model.\n",
      "2023-10-17 07:43:33.414826 [W:onnxruntime:, graph.cc:3543 CleanUnusedInitializersAndNodeArgs] Removing initializer '/model/decoder/layers.11/encoder_attn_layer_norm/Constant_1_output_0'. It is not used by any node and should be removed from the model.\n",
      "2023-10-17 07:43:33.414831 [W:onnxruntime:, graph.cc:3543 CleanUnusedInitializersAndNodeArgs] Removing initializer '/model/decoder/layers.1/self_attn_layer_norm/Constant_1_output_0'. It is not used by any node and should be removed from the model.\n",
      "2023-10-17 07:43:33.414835 [W:onnxruntime:, graph.cc:3543 CleanUnusedInitializersAndNodeArgs] Removing initializer '/model/decoder/layers.0/self_attn_layer_norm/Constant_output_0'. It is not used by any node and should be removed from the model.\n",
      "2023-10-17 07:43:33.414841 [W:onnxruntime:, graph.cc:3543 CleanUnusedInitializersAndNodeArgs] Removing initializer '/model/decoder/layers.2/encoder_attn_layer_norm/Constant_output_0'. It is not used by any node and should be removed from the model.\n",
      "2023-10-17 07:43:33.414846 [W:onnxruntime:, graph.cc:3543 CleanUnusedInitializersAndNodeArgs] Removing initializer '/model/decoder/layers.0/encoder_attn_layer_norm/Constant_output_0'. It is not used by any node and should be removed from the model.\n",
      "2023-10-17 07:43:33.414850 [W:onnxruntime:, graph.cc:3543 CleanUnusedInitializersAndNodeArgs] Removing initializer '/model/decoder/layers.8/self_attn_layer_norm/Constant_1_output_0'. It is not used by any node and should be removed from the model.\n",
      "2023-10-17 07:43:33.414853 [W:onnxruntime:, graph.cc:3543 CleanUnusedInitializersAndNodeArgs] Removing initializer '/model/decoder/layers.3/final_layer_norm/Constant_output_0'. It is not used by any node and should be removed from the model.\n",
      "2023-10-17 07:43:33.414858 [W:onnxruntime:, graph.cc:3543 CleanUnusedInitializersAndNodeArgs] Removing initializer '/model/decoder/layers.2/self_attn_layer_norm/Constant_output_0'. It is not used by any node and should be removed from the model.\n",
      "2023-10-17 07:43:33.414863 [W:onnxruntime:, graph.cc:3543 CleanUnusedInitializersAndNodeArgs] Removing initializer '/model/decoder/layers.10/final_layer_norm/Constant_output_0'. It is not used by any node and should be removed from the model.\n",
      "2023-10-17 07:43:33.414866 [W:onnxruntime:, graph.cc:3543 CleanUnusedInitializersAndNodeArgs] Removing initializer '/model/decoder/layers.4/self_attn_layer_norm/Constant_1_output_0'. It is not used by any node and should be removed from the model.\n",
      "2023-10-17 07:43:33.416247 [W:onnxruntime:, graph.cc:3543 CleanUnusedInitializersAndNodeArgs] Removing initializer '/model/decoder/layer_norm/Constant_output_0'. It is not used by any node and should be removed from the model.\n",
      "2023-10-17 07:43:33.416301 [W:onnxruntime:, graph.cc:3543 CleanUnusedInitializersAndNodeArgs] Removing initializer '/model/decoder/layers.11/final_layer_norm/Constant_1_output_0'. It is not used by any node and should be removed from the model.\n",
      "2023-10-17 07:43:33.416310 [W:onnxruntime:, graph.cc:3543 CleanUnusedInitializersAndNodeArgs] Removing initializer '/model/decoder/layers.11/encoder_attn_layer_norm/Constant_output_0'. It is not used by any node and should be removed from the model.\n",
      "2023-10-17 07:43:33.416316 [W:onnxruntime:, graph.cc:3543 CleanUnusedInitializersAndNodeArgs] Removing initializer '/model/decoder/layers.11/self_attn_layer_norm/Constant_1_output_0'. It is not used by any node and should be removed from the model.\n",
      "2023-10-17 07:43:33.416320 [W:onnxruntime:, graph.cc:3543 CleanUnusedInitializersAndNodeArgs] Removing initializer '/model/decoder/layers.10/final_layer_norm/Constant_1_output_0'. It is not used by any node and should be removed from the model.\n",
      "2023-10-17 07:43:33.416327 [W:onnxruntime:, graph.cc:3543 CleanUnusedInitializersAndNodeArgs] Removing initializer '/model/decoder/layers.10/encoder_attn_layer_norm/Constant_1_output_0'. It is not used by any node and should be removed from the model.\n",
      "2023-10-17 07:43:33.416333 [W:onnxruntime:, graph.cc:3543 CleanUnusedInitializersAndNodeArgs] Removing initializer '/model/decoder/layers.10/self_attn_layer_norm/Constant_1_output_0'. It is not used by any node and should be removed from the model.\n",
      "2023-10-17 07:43:33.416337 [W:onnxruntime:, graph.cc:3543 CleanUnusedInitializersAndNodeArgs] Removing initializer '/model/decoder/layers.10/self_attn_layer_norm/Constant_output_0'. It is not used by any node and should be removed from the model.\n",
      "2023-10-17 07:43:33.416341 [W:onnxruntime:, graph.cc:3543 CleanUnusedInitializersAndNodeArgs] Removing initializer '/model/decoder/layers.9/final_layer_norm/Constant_1_output_0'. It is not used by any node and should be removed from the model.\n",
      "2023-10-17 07:43:33.416345 [W:onnxruntime:, graph.cc:3543 CleanUnusedInitializersAndNodeArgs] Removing initializer '/model/decoder/layers.9/final_layer_norm/Constant_output_0'. It is not used by any node and should be removed from the model.\n",
      "2023-10-17 07:43:33.416351 [W:onnxruntime:, graph.cc:3543 CleanUnusedInitializersAndNodeArgs] Removing initializer '/model/decoder/layers.9/encoder_attn_layer_norm/Constant_1_output_0'. It is not used by any node and should be removed from the model.\n",
      "2023-10-17 07:43:33.416358 [W:onnxruntime:, graph.cc:3543 CleanUnusedInitializersAndNodeArgs] Removing initializer '/model/decoder/layers.9/self_attn_layer_norm/Constant_1_output_0'. It is not used by any node and should be removed from the model.\n",
      "2023-10-17 07:43:33.416362 [W:onnxruntime:, graph.cc:3543 CleanUnusedInitializersAndNodeArgs] Removing initializer '/model/decoder/layers.9/self_attn_layer_norm/Constant_output_0'. It is not used by any node and should be removed from the model.\n",
      "2023-10-17 07:43:33.416365 [W:onnxruntime:, graph.cc:3543 CleanUnusedInitializersAndNodeArgs] Removing initializer '/model/decoder/layers.8/final_layer_norm/Constant_1_output_0'. It is not used by any node and should be removed from the model.\n",
      "2023-10-17 07:43:33.416369 [W:onnxruntime:, graph.cc:3543 CleanUnusedInitializersAndNodeArgs] Removing initializer '/model/decoder/layers.8/final_layer_norm/Constant_output_0'. It is not used by any node and should be removed from the model.\n",
      "2023-10-17 07:43:33.416373 [W:onnxruntime:, graph.cc:3543 CleanUnusedInitializersAndNodeArgs] Removing initializer '/model/decoder/layers.8/encoder_attn_layer_norm/Constant_1_output_0'. It is not used by any node and should be removed from the model.\n",
      "2023-10-17 07:43:33.416380 [W:onnxruntime:, graph.cc:3543 CleanUnusedInitializersAndNodeArgs] Removing initializer '/model/decoder/layers.7/final_layer_norm/Constant_1_output_0'. It is not used by any node and should be removed from the model.\n",
      "2023-10-17 07:43:33.416384 [W:onnxruntime:, graph.cc:3543 CleanUnusedInitializersAndNodeArgs] Removing initializer '/model/decoder/layers.7/final_layer_norm/Constant_output_0'. It is not used by any node and should be removed from the model.\n",
      "2023-10-17 07:43:33.416389 [W:onnxruntime:, graph.cc:3543 CleanUnusedInitializersAndNodeArgs] Removing initializer '/model/decoder/layers.7/encoder_attn_layer_norm/Constant_output_0'. It is not used by any node and should be removed from the model.\n",
      "2023-10-17 07:43:33.416395 [W:onnxruntime:, graph.cc:3543 CleanUnusedInitializersAndNodeArgs] Removing initializer '/model/decoder/layers.7/self_attn_layer_norm/Constant_1_output_0'. It is not used by any node and should be removed from the model.\n",
      "2023-10-17 07:43:33.416399 [W:onnxruntime:, graph.cc:3543 CleanUnusedInitializersAndNodeArgs] Removing initializer '/model/decoder/layers.7/self_attn_layer_norm/Constant_output_0'. It is not used by any node and should be removed from the model.\n",
      "2023-10-17 07:43:33.416403 [W:onnxruntime:, graph.cc:3543 CleanUnusedInitializersAndNodeArgs] Removing initializer '/model/decoder/layers.6/final_layer_norm/Constant_output_0'. It is not used by any node and should be removed from the model.\n",
      "2023-10-17 07:43:33.416406 [W:onnxruntime:, graph.cc:3543 CleanUnusedInitializersAndNodeArgs] Removing initializer '/model/decoder/layer_norm/Constant_1_output_0'. It is not used by any node and should be removed from the model.\n",
      "2023-10-17 07:43:33.416411 [W:onnxruntime:, graph.cc:3543 CleanUnusedInitializersAndNodeArgs] Removing initializer '/model/decoder/layers.9/encoder_attn_layer_norm/Constant_output_0'. It is not used by any node and should be removed from the model.\n",
      "2023-10-17 07:43:33.416417 [W:onnxruntime:, graph.cc:3543 CleanUnusedInitializersAndNodeArgs] Removing initializer '/model/decoder/layers.6/encoder_attn_layer_norm/Constant_1_output_0'. It is not used by any node and should be removed from the model.\n",
      "2023-10-17 07:43:33.416421 [W:onnxruntime:, graph.cc:3543 CleanUnusedInitializersAndNodeArgs] Removing initializer '/model/decoder/layers.6/encoder_attn_layer_norm/Constant_output_0'. It is not used by any node and should be removed from the model.\n",
      "2023-10-17 07:43:33.416430 [W:onnxruntime:, graph.cc:3543 CleanUnusedInitializersAndNodeArgs] Removing initializer '/model/decoder/layers.8/self_attn_layer_norm/Constant_output_0'. It is not used by any node and should be removed from the model.\n",
      "2023-10-17 07:43:33.416433 [W:onnxruntime:, graph.cc:3543 CleanUnusedInitializersAndNodeArgs] Removing initializer '/model/decoder/layers.5/final_layer_norm/Constant_1_output_0'. It is not used by any node and should be removed from the model.\n",
      "2023-10-17 07:43:33.416439 [W:onnxruntime:, graph.cc:3543 CleanUnusedInitializersAndNodeArgs] Removing initializer '/model/decoder/layers.7/encoder_attn_layer_norm/Constant_1_output_0'. It is not used by any node and should be removed from the model.\n",
      "2023-10-17 07:43:33.416443 [W:onnxruntime:, graph.cc:3543 CleanUnusedInitializersAndNodeArgs] Removing initializer '/model/decoder/layers.5/encoder_attn_layer_norm/Constant_1_output_0'. It is not used by any node and should be removed from the model.\n",
      "2023-10-17 07:43:33.416447 [W:onnxruntime:, graph.cc:3543 CleanUnusedInitializersAndNodeArgs] Removing initializer '/model/decoder/layers.5/encoder_attn_layer_norm/Constant_output_0'. It is not used by any node and should be removed from the model.\n",
      "2023-10-17 07:43:33.416454 [W:onnxruntime:, graph.cc:3543 CleanUnusedInitializersAndNodeArgs] Removing initializer '/model/decoder/layers.0/encoder_attn_layer_norm/Constant_1_output_0'. It is not used by any node and should be removed from the model.\n",
      "2023-10-17 07:43:33.416460 [W:onnxruntime:, graph.cc:3543 CleanUnusedInitializersAndNodeArgs] Removing initializer '/model/decoder/layers.5/self_attn_layer_norm/Constant_1_output_0'. It is not used by any node and should be removed from the model.\n",
      "2023-10-17 07:43:33.416464 [W:onnxruntime:, graph.cc:3543 CleanUnusedInitializersAndNodeArgs] Removing initializer '/model/decoder/layers.5/self_attn_layer_norm/Constant_output_0'. It is not used by any node and should be removed from the model.\n",
      "2023-10-17 07:43:33.416468 [W:onnxruntime:, graph.cc:3543 CleanUnusedInitializersAndNodeArgs] Removing initializer '/model/decoder/layers.4/final_layer_norm/Constant_1_output_0'. It is not used by any node and should be removed from the model.\n",
      "2023-10-17 07:43:33.416472 [W:onnxruntime:, graph.cc:3543 CleanUnusedInitializersAndNodeArgs] Removing initializer '/model/decoder/layers.4/final_layer_norm/Constant_output_0'. It is not used by any node and should be removed from the model.\n",
      "2023-10-17 07:43:33.416477 [W:onnxruntime:, graph.cc:3543 CleanUnusedInitializersAndNodeArgs] Removing initializer '/model/decoder/layers.6/self_attn_layer_norm/Constant_output_0'. It is not used by any node and should be removed from the model.\n",
      "2023-10-17 07:43:33.416483 [W:onnxruntime:, graph.cc:3543 CleanUnusedInitializersAndNodeArgs] Removing initializer '/model/decoder/layers.4/encoder_attn_layer_norm/Constant_1_output_0'. It is not used by any node and should be removed from the model.\n",
      "2023-10-17 07:43:33.416512 [W:onnxruntime:, graph.cc:3543 CleanUnusedInitializersAndNodeArgs] Removing initializer '/model/decoder/layers.10/final_layer_norm/Constant_output_0'. It is not used by any node and should be removed from the model.\n",
      "2023-10-17 07:43:33.416543 [W:onnxruntime:, graph.cc:3543 CleanUnusedInitializersAndNodeArgs] Removing initializer '/model/decoder/layers.4/self_attn_layer_norm/Constant_1_output_0'. It is not used by any node and should be removed from the model.\n",
      "2023-10-17 07:43:33.416548 [W:onnxruntime:, graph.cc:3543 CleanUnusedInitializersAndNodeArgs] Removing initializer '/model/decoder/layers.3/final_layer_norm/Constant_1_output_0'. It is not used by any node and should be removed from the model.\n",
      "2023-10-17 07:43:33.416555 [W:onnxruntime:, graph.cc:3543 CleanUnusedInitializersAndNodeArgs] Removing initializer '/model/decoder/layers.3/encoder_attn_layer_norm/Constant_1_output_0'. It is not used by any node and should be removed from the model.\n",
      "2023-10-17 07:43:33.416560 [W:onnxruntime:, graph.cc:3543 CleanUnusedInitializersAndNodeArgs] Removing initializer '/model/decoder/layers.3/encoder_attn_layer_norm/Constant_output_0'. It is not used by any node and should be removed from the model.\n",
      "2023-10-17 07:43:33.416564 [W:onnxruntime:, graph.cc:3543 CleanUnusedInitializersAndNodeArgs] Removing initializer '/model/decoder/layers.1/encoder_attn_layer_norm/Constant_output_0'. It is not used by any node and should be removed from the model.\n",
      "2023-10-17 07:43:33.416568 [W:onnxruntime:, graph.cc:3543 CleanUnusedInitializersAndNodeArgs] Removing initializer '/model/decoder/layers.8/encoder_attn_layer_norm/Constant_output_0'. It is not used by any node and should be removed from the model.\n",
      "2023-10-17 07:43:33.416573 [W:onnxruntime:, graph.cc:3543 CleanUnusedInitializersAndNodeArgs] Removing initializer '/model/decoder/layers.4/self_attn_layer_norm/Constant_output_0'. It is not used by any node and should be removed from the model.\n",
      "2023-10-17 07:43:33.416578 [W:onnxruntime:, graph.cc:3543 CleanUnusedInitializersAndNodeArgs] Removing initializer '/model/decoder/layers.3/self_attn_layer_norm/Constant_1_output_0'. It is not used by any node and should be removed from the model.\n",
      "2023-10-17 07:43:33.416583 [W:onnxruntime:, graph.cc:3543 CleanUnusedInitializersAndNodeArgs] Removing initializer '/model/decoder/layers.3/self_attn_layer_norm/Constant_output_0'. It is not used by any node and should be removed from the model.\n",
      "2023-10-17 07:43:33.416591 [W:onnxruntime:, graph.cc:3543 CleanUnusedInitializersAndNodeArgs] Removing initializer '/model/decoder/layers.3/final_layer_norm/Constant_output_0'. It is not used by any node and should be removed from the model.\n",
      "2023-10-17 07:43:33.416595 [W:onnxruntime:, graph.cc:3543 CleanUnusedInitializersAndNodeArgs] Removing initializer '/model/decoder/layers.8/self_attn_layer_norm/Constant_1_output_0'. It is not used by any node and should be removed from the model.\n",
      "2023-10-17 07:43:33.416599 [W:onnxruntime:, graph.cc:3543 CleanUnusedInitializersAndNodeArgs] Removing initializer '/model/decoder/layers.2/encoder_attn_layer_norm/Constant_1_output_0'. It is not used by any node and should be removed from the model.\n",
      "2023-10-17 07:43:33.416604 [W:onnxruntime:, graph.cc:3543 CleanUnusedInitializersAndNodeArgs] Removing initializer '/model/decoder/layers.2/final_layer_norm/Constant_1_output_0'. It is not used by any node and should be removed from the model.\n",
      "2023-10-17 07:43:33.416608 [W:onnxruntime:, graph.cc:3543 CleanUnusedInitializersAndNodeArgs] Removing initializer '/model/decoder/layers.1/encoder_attn_layer_norm/Constant_1_output_0'. It is not used by any node and should be removed from the model.\n",
      "2023-10-17 07:43:33.416612 [W:onnxruntime:, graph.cc:3543 CleanUnusedInitializersAndNodeArgs] Removing initializer '/model/decoder/layers.2/final_layer_norm/Constant_output_0'. It is not used by any node and should be removed from the model.\n",
      "2023-10-17 07:43:33.416659 [W:onnxruntime:, graph.cc:3543 CleanUnusedInitializersAndNodeArgs] Removing initializer '/model/decoder/layers.2/self_attn_layer_norm/Constant_1_output_0'. It is not used by any node and should be removed from the model.\n",
      "2023-10-17 07:43:33.416663 [W:onnxruntime:, graph.cc:3543 CleanUnusedInitializersAndNodeArgs] Removing initializer '/model/decoder/layers.1/final_layer_norm/Constant_1_output_0'. It is not used by any node and should be removed from the model.\n",
      "2023-10-17 07:43:33.416667 [W:onnxruntime:, graph.cc:3543 CleanUnusedInitializersAndNodeArgs] Removing initializer '/model/decoder/layers.1/final_layer_norm/Constant_output_0'. It is not used by any node and should be removed from the model.\n",
      "2023-10-17 07:43:33.416700 [W:onnxruntime:, graph.cc:3543 CleanUnusedInitializersAndNodeArgs] Removing initializer '/model/decoder/layers.11/final_layer_norm/Constant_output_0'. It is not used by any node and should be removed from the model.\n",
      "2023-10-17 07:43:33.416708 [W:onnxruntime:, graph.cc:3543 CleanUnusedInitializersAndNodeArgs] Removing initializer '/model/decoder/layers.6/final_layer_norm/Constant_1_output_0'. It is not used by any node and should be removed from the model.\n",
      "2023-10-17 07:43:33.416714 [W:onnxruntime:, graph.cc:3543 CleanUnusedInitializersAndNodeArgs] Removing initializer '/model/decoder/layers.0/final_layer_norm/Constant_output_0'. It is not used by any node and should be removed from the model.\n",
      "2023-10-17 07:43:33.416719 [W:onnxruntime:, graph.cc:3543 CleanUnusedInitializersAndNodeArgs] Removing initializer '/model/decoder/layers.10/encoder_attn_layer_norm/Constant_output_0'. It is not used by any node and should be removed from the model.\n",
      "2023-10-17 07:43:33.416723 [W:onnxruntime:, graph.cc:3543 CleanUnusedInitializersAndNodeArgs] Removing initializer '/model/decoder/layers.0/self_attn_layer_norm/Constant_1_output_0'. It is not used by any node and should be removed from the model.\n",
      "2023-10-17 07:43:33.416728 [W:onnxruntime:, graph.cc:3543 CleanUnusedInitializersAndNodeArgs] Removing initializer '/model/decoder/layers.5/final_layer_norm/Constant_output_0'. It is not used by any node and should be removed from the model.\n",
      "2023-10-17 07:43:33.416732 [W:onnxruntime:, graph.cc:3543 CleanUnusedInitializersAndNodeArgs] Removing initializer '/model/decoder/layers.6/self_attn_layer_norm/Constant_1_output_0'. It is not used by any node and should be removed from the model.\n",
      "2023-10-17 07:43:33.416736 [W:onnxruntime:, graph.cc:3543 CleanUnusedInitializersAndNodeArgs] Removing initializer '/model/decoder/layers.0/final_layer_norm/Constant_1_output_0'. It is not used by any node and should be removed from the model.\n",
      "2023-10-17 07:43:33.416742 [W:onnxruntime:, graph.cc:3543 CleanUnusedInitializersAndNodeArgs] Removing initializer '/model/decoder/layers.1/self_attn_layer_norm/Constant_output_0'. It is not used by any node and should be removed from the model.\n",
      "2023-10-17 07:43:33.416746 [W:onnxruntime:, graph.cc:3543 CleanUnusedInitializersAndNodeArgs] Removing initializer '/model/decoder/layers.4/encoder_attn_layer_norm/Constant_output_0'. It is not used by any node and should be removed from the model.\n",
      "2023-10-17 07:43:33.416750 [W:onnxruntime:, graph.cc:3543 CleanUnusedInitializersAndNodeArgs] Removing initializer '/model/decoder/layers.11/self_attn_layer_norm/Constant_output_0'. It is not used by any node and should be removed from the model.\n",
      "2023-10-17 07:43:33.416755 [W:onnxruntime:, graph.cc:3543 CleanUnusedInitializersAndNodeArgs] Removing initializer '/model/decoder/layers.1/self_attn_layer_norm/Constant_1_output_0'. It is not used by any node and should be removed from the model.\n",
      "2023-10-17 07:43:33.416759 [W:onnxruntime:, graph.cc:3543 CleanUnusedInitializersAndNodeArgs] Removing initializer '/model/decoder/layers.2/self_attn_layer_norm/Constant_output_0'. It is not used by any node and should be removed from the model.\n",
      "2023-10-17 07:43:33.416763 [W:onnxruntime:, graph.cc:3543 CleanUnusedInitializersAndNodeArgs] Removing initializer '/model/decoder/layers.0/self_attn_layer_norm/Constant_output_0'. It is not used by any node and should be removed from the model.\n",
      "2023-10-17 07:43:33.416769 [W:onnxruntime:, graph.cc:3543 CleanUnusedInitializersAndNodeArgs] Removing initializer '/model/decoder/layers.2/encoder_attn_layer_norm/Constant_output_0'. It is not used by any node and should be removed from the model.\n",
      "2023-10-17 07:43:33.416773 [W:onnxruntime:, graph.cc:3543 CleanUnusedInitializersAndNodeArgs] Removing initializer '/model/decoder/layers.11/encoder_attn_layer_norm/Constant_1_output_0'. It is not used by any node and should be removed from the model.\n",
      "2023-10-17 07:43:33.416779 [W:onnxruntime:, graph.cc:3543 CleanUnusedInitializersAndNodeArgs] Removing initializer '/model/decoder/layers.0/encoder_attn_layer_norm/Constant_output_0'. It is not used by any node and should be removed from the model.\n"
     ]
    }
   ],
   "source": [
    "quantized_model = ORTModelForSeq2SeqLM.from_pretrained(quantized_model_path, \n",
    "                                                       decoder_file_name='decoder_model_quantized.onnx',\n",
    "                                                       encoder_file_name='encoder_model_quantized.onnx',)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantized_pipeline = pipeline(\"translation_en_to_sw\", model=quantized_model, tokenizer=tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "translated_text_quantized = quantized_pipeline(text_to_translate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'translation_text': 'Jina langu ni Espoir Murhabazi, Mimi ni mhandisi wa programu za kompyuta kutoka Kongo DRC lakini ninaishi Uingereza'}]\n"
     ]
    }
   ],
   "source": [
    "print(translated_text_quantized)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The quantization seems to reduce the size of the model but keeping the same performance, as per the documentaiton and experience performed on other models, we need to perform the quantization on other model to check for the performance."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With  our model quantized let us move to the next step which is a deployment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deploy the Model for inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point we have our model quantized and saved as onnx format. We will now deploy it to a production server using triton inference server. \n",
    "In the first section we will deploy with triton server as a docker container, and then we will use Kserve to deploy it to the kubernetes deployment environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is Triton Inference Server?\n",
    "\n",
    "Triton is software tool for deploying machine learning models for inference. It is designed to produce high quality inference across different hardware platform either GPU or CPU. It also support inference across cloud, data center and embedded devices.\n",
    "One of the advantage about the triton server I found, is the fact that it support dynamic batching and concurrent model execution\n",
    "\n",
    "- Concurency model execution is the capacity to run simultaneously multiple models on the same GPU or on multiple GPUs.\n",
    "\n",
    "- Dynamic batching, for model that support batching, which is the case for deep learning models, triton implements scheduling and batching algorithms that combine individual requests together to improve inference throughput.\n",
    "\n",
    "\n",
    "### Triton Server Backend\n",
    "Triton support different backend to execute the model. A backend is a wrapper around a deep learning framework like Pytorch , TensorFlow, TensorRT or ONNX Runtime.\n",
    "Two backend type interested us for this post, the Python Backend and the ONNX runtime backend. \n",
    "\n",
    "The onnx runtime backend is used to execute onnx models, the python backend allow to write the  model logic in python. \n",
    "\n",
    "In  this post we will be focused on the ONNX and the Python backend.\n",
    "\n",
    "I decided to go with the python backend because I struggled to deploy the encoder decode model using ensemble of ONNX model. I still have a question in progress on [StackOverlow](https://stackoverflow.com/q/76638766/4683950).  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Uploading the Model to Repository.\n",
    "\n",
    "The first step before using our model is to upload it to the model repository, for starting we will be using our local storage as model repository but later we will use a static storage such as google cloud or AWS S3 to host our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration\n",
    "\n",
    "The first step to deploy our model in triton is to configure it.\n",
    "\n",
    "The configuration setup the model and define the input shape and the output shape of our models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load ./triton_model_repository/m2m100_translation_model/config.pbtxt\n",
    "name: \"m2m100_translation_model\"\n",
    "backend: \"python\"\n",
    "max_batch_size: 0\n",
    "input [\n",
    "  {\n",
    "    name: \"input_ids\"\n",
    "    data_type: TYPE_INT64\n",
    "    dims: [ -1, -1 ]\n",
    "  },\n",
    "{\n",
    "    name: \"attention_mask\"\n",
    "    data_type: TYPE_INT64\n",
    "    dims: [ -1, -1 ]\n",
    "  }\n",
    "]\n",
    "output [\n",
    "    {\n",
    "    name: \"generated_indices\"\n",
    "    data_type: TYPE_FP32\n",
    "    dims: [ -1, -1 ]\n",
    "  }\n",
    "]\n",
    "\n",
    "instance_group [\n",
    "    {\n",
    "      count: 1\n",
    "      kind: KIND_CPU\n",
    "    }\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above configuration, we can see that the model is expecting two inputs, the inputs ids, and the attention masks, and it returns the generated input indices.\n",
    "\n",
    "The input id and the attention masks are the outputs from the tokenisation process. The generated indices are the tokenised output indices, that will be decoded to find our generated indices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The configuratin file needs to be save at the root file of our model repository."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create the load model script\n",
    "\n",
    "The load model script is the python script that load our model before and run it for inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load ./triton_model_repository/m2m100_translation_model/1/model.py\n",
    "from typing import Dict, List\n",
    "import triton_python_backend_utils as pb_utils\n",
    "from pathlib import Path\n",
    "from optimum.onnxruntime import ORTModelForSeq2SeqLM\n",
    "import torch\n",
    "\n",
    "TOKENIZER_SW_LANG_CODE_TO_ID = 128088\n",
    "\n",
    "\n",
    "class TritonPythonModel:\n",
    "\n",
    "    def initialize(self, args: Dict[str, str]) -> None:\n",
    "        \"\"\"\n",
    "        Initialize the tokenization process\n",
    "        :param args: arguments from Triton config file\n",
    "        \"\"\"\n",
    "        current_path: str = Path(args[\"model_repository\"]).parent.absolute()\n",
    "        model_path = current_path.joinpath(\"m2m100_translation_model\", \"1\", \"m2m100_418M_en_swa_rel_news_quantized\")\n",
    "        self.device = \"cpu\" if args[\"model_instance_kind\"] == \"CPU\" else \"cuda\"\n",
    "        # more variables in https://github.com/triton-inference-server/python_backend/blob/main/src/python.cc\n",
    "        self.model = ORTModelForSeq2SeqLM.from_pretrained(model_path,\n",
    "                                                          decoder_file_name=\"decoder_model_quantized.onnx\",\n",
    "                                                          encoder_file_name=\"encoder_model_quantized.onnx\")\n",
    "        if self.device == \"cuda\":\n",
    "            self.model = self.model.cuda()\n",
    "        print(\"TritonPythonModel initialized\")\n",
    "\n",
    "    def execute(self, requests) -> \"List[List[pb_utils.Tensor]]\":\n",
    "        \"\"\"\n",
    "        Parse and tokenize each request\n",
    "        :param requests: 1 or more requests received by Triton server.\n",
    "        :return: text as input tensors\n",
    "        \"\"\"\n",
    "        responses = []\n",
    "        # for loop for batch requests (disabled in our case)\n",
    "        for request in requests:\n",
    "            # binary data typed back to string\n",
    "            input_ids = pb_utils.get_input_tensor_by_name(request, \"input_ids\").as_numpy()\n",
    "            attention_masks = pb_utils.get_input_tensor_by_name(request, \"attention_mask\").as_numpy()\n",
    "            input_ids = torch.as_tensor(input_ids, dtype=torch.int64)\n",
    "            attention_masks = torch.as_tensor(attention_masks, dtype=torch.int64)\n",
    "            if self.device == \"cuda\":\n",
    "                input_ids = input_ids.to(\"cuda\")\n",
    "                attention_masks = attention_masks.to(\"cuda\")\n",
    "            model_inputs = {\"input_ids\": input_ids, \"attention_mask\": attention_masks}\n",
    "            generated_indices = self.model.generate(**model_inputs,\n",
    "                                                    forced_bos_token_id=TOKENIZER_SW_LANG_CODE_TO_ID)\n",
    "            tensor_output = pb_utils.Tensor(\"generated_indices\", generated_indices.numpy())\n",
    "            responses.append(tensor_output)\n",
    "        responses = [pb_utils.InferenceResponse(output_tensors=responses)]\n",
    "        return responses\n",
    "    \n",
    "    def finalize(self):\n",
    "        \"\"\"`finalize` is called only once when the model is being unloaded.\n",
    "        Implementing `finalize` function is optional. This function allows\n",
    "        the model to perform any necessary clean ups before exit.\n",
    "        \"\"\"\n",
    "        print('Cleaning up...')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model contains a class with two methods:\n",
    "\n",
    "- Initialize: The initialize method use the ORT model to load the model in the memory!\n",
    "- The execute method parse and tokenize each request receive by the triton server. It call the generate method on the input of the request and return the generated text indices. This text will be later decode by the tokenizer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If our configuration is done properly and the model was saved properly, we should have a model repository that looks like this:\n",
    "\n",
    "```\n",
    "triton_model_repository\n",
    "└── m2m100_translation_model\n",
    "    ├── 1\n",
    "    │   ├── m2m100_418M_en_swa_rel_news_quantized\n",
    "    │   │   ├── config.json\n",
    "    │   │   ├── decoder_model_quantized.onnx\n",
    "    │   │   ├── decoder_with_past_model_quantized.onnx\n",
    "    │   │   ├── encoder_model_quantized.onnx\n",
    "    │   │   └── ort_config.json\n",
    "    │   └── model.py\n",
    "    └── config.pbtxt\n",
    "```\n",
    "\n",
    "Make sure that you have the file located at the precise location as me in order to be able to run the code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Launching the docker image\n",
    "\n",
    "If you look carefully at the code for our python model you can see that the model, is importing the ONNX runtime! However that runtime is not installed in the base triton server image. Reason why we decided to build our own triton image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load Dockerfile\n",
    "# Use the base image\n",
    "FROM nvcr.io/nvidia/tritonserver:23.06-py3\n",
    "\n",
    "# Install the required Python packages\n",
    "RUN pip install optimum==1.9.0 onnxruntime==1.15.1 onnx==1.14.0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above code show how we build our docker image.\n",
    "We are using the base tritonserver image and then we add the different packages we need to run our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we can build our model using:\n",
    "\n",
    "`docker build -t espymur/triton-onnx:dev  -f Dockerfile .`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please not that the image is huge, around 15 Gb, in the next post I will try to optimize the image size by using technic suggested in the documentation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "`docker run --rm -p 8000:8000 -p 8001:8001 -p 8002:8002  --shm-size 128M -v ${PWD}/triton_model_repository:/models  espymur/triton-onnx:dev tritonserver --model-repository=/models`\n",
    "\n",
    "- This command run the docker container and map the the port 8000, 8001, 8002 to 8000, 8001, 8002 of our local machine.\n",
    "\n",
    "- It then create a volume that maps the `${PWD}/triton_model_repository` path from our local machine to /models in the container.\n",
    "\n",
    "- It is also using a shared memory of 128 Mb.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this model we can see that our model is running and we can perform inference without any problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, we have got our model running inside the docker container, the next step will be to make inference requests. Let see how we can achieve that."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making Inference Requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model is now updated and saved as  triton backend model we will apply tokenization offline and query the model with the tokenized words and the attention mask. \n",
    "The model will return the indices of the translated test, we will use the tokenizer again to decode the indices and produce the output.\n",
    "\n",
    "We can later have the tokenizer as a separate service people can interact with using http."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"masakhane/m2m100_418M_en_swa_rel_news\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tritonclient.http as httpclient\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The HTTP client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = httpclient.InferenceServerClient(url=\"localhost:8000\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This line create the client object we will be using to interact with our server. To create the client object we are passing the url of the inference service as parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = httpclient.InferInput(\"input_ids\", shape=(-1,1) , datatype=\"TYPE_INT64\",)\n",
    "attention_mask = httpclient.InferInput(\"attention_mask\", shape=(-1,1) , datatype=\"TYPE_INT64\",)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = httpclient.InferRequestedOutput(\"generated_indices\", binary_data=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To prepare our model input we are using the triton client library. \n",
    "The above code create two objects for the input id and the attention mask respectively! We can specify the shape our the element and their datatype when creating the code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additionally to our inputs and outputs, we will need some utility function to perform the tokenization. Here are those functions:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Utilities Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tokenizer(model_name):\n",
    "    \"\"\"Returns a tokenizer for a given model name\n",
    "\n",
    "    Args:\n",
    "        model_name (_type_): _description_\n",
    "\n",
    "    Returns:\n",
    "        _type_: _description_\n",
    "    \"\"\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    return tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple, List\n",
    "\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/esp.py/Projects/Personal/masakhane-web/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def tokenize_text(tokenizer: AutoTokenizer, text:str) -> Tuple[np.ndarray , np.ndarray]:\n",
    "    tokenized_text = tokenizer(text, padding=True, return_tensors=\"np\")\n",
    "    return tokenized_text.input_ids, tokenized_text.attention_mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_inference_input(input_ids: np.ndarray, attention_mask: np.ndarray) -> List[httpclient.InferInput]:\n",
    "    \"\"\"\n",
    "    Generate inference inputs for Triton server\n",
    "\n",
    "    Args:\n",
    "        input_ids (np.ndarray): _description_\n",
    "        attention_mask (np.ndarray): _description_\n",
    "\n",
    "    Returns:\n",
    "        List[httpclient.InferInput]: _description_\n",
    "    \"\"\"\n",
    "    inputs = []\n",
    "    inputs.append(httpclient.InferInput(\"input_ids\", input_ids.shape, \"INT64\"))\n",
    "    inputs.append(httpclient.InferInput(\"attention_mask\", attention_mask.shape, \"INT64\"))\n",
    "\n",
    "    inputs[0].set_data_from_numpy(input_ids.astype(np.int64), binary_data=False)\n",
    "    inputs[1].set_data_from_numpy(attention_mask.astype(np.int64), binary_data=False)\n",
    "    return inputs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = [\"I am learning how to use Triton Server for Machine Learning\", \"Hello, my name is Espoir Murhabazi,  I am a Software Engineer from Congo DRC but living in UK\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = get_tokenizer(MODEL_NAME)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids, attention_mask = tokenize_text(tokenizer, text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_inputs = generate_inference_input(input_ids, attention_mask)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With our input prepared we can now make an inference request to our server. Here is the code we will be using to make the inference request."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<HTTPSocketPoolResponse status=200 headers={'content-type': 'application/json', 'content-length': '453'}>\n",
      "**********\n"
     ]
    }
   ],
   "source": [
    "results = client.infer(model_name=\"m2m100_translation_model\", inputs=inference_inputs, outputs=[outputs])\n",
    "inference_output = results.as_numpy('generated_indices')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If everything goes as planned, we should be able to see the inference response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[     2, 128088,  71714,    720,  12089,    438,  51759,    377,\n",
       "           102,  28668,  21552,  37578,  53140,    311,    103,   2447,\n",
       "            82,   2786,   3194,    720,  12089,    438,  28668,  21552,\n",
       "         55125,    360,      2,      1,      1,      1,      1,      1,\n",
       "             1,      1],\n",
       "       [     2, 128088,    298,    260, 118240,    243,   6209,  18234,\n",
       "         10749,   8612,   2956,      4,    100,   1123,    243,    172,\n",
       "          8245,    649,    311,  29574,    181, 112209,   1777,  14903,\n",
       "           129,   9382,  22310,    247,  24109,  67338,   7022,    352,\n",
       "         98264,      2]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inference_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded_output = tokenizer.batch_decode(inference_output, skip_special_tokens=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Ninajifunza Jinsi ya Kutumia Mtandao wa Triton ili Kujifunza Kutumia Mashine',\n",
       " 'Jina langu ni Espoir Murhabazi, Mimi ni mhandisi wa programu za kompyuta kutoka Kongo DRC lakini ninaishi Uingereza']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoded_output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the decoded output, we can see that our inference server is working!\n",
    "In this post, we saw how we can start form a raw translation model from huggingface, we then quantized it to reduce it's size, and finally deployed the model on a triton server to perform inference.\n",
    "In the second part of this blog we will learn how to scale the whole prototype and build an end to end pipeline using kubernetes and Kserve."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e7d420a2576d2f2cf4aee17bb1c719cb2b545f2d9fd7bdced2270e528bc643b9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
