{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference with the updated model.\n",
    "\n",
    "The model is now updated and saved as  triton backend model we will apply tokenization offline and query the model with the tokenized words and the attention mask. \n",
    "The model will return the indices of the translated test, we will use the tokenizer again to decode the indices and produce the output.\n",
    "\n",
    "We can later have the tokenizer as a separate service people can interact with using http."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"masakhane/m2m100_418M_en_swa_rel_news\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/esp.py/Projects/Personal/masakhane-web/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tritonclient.http as httpclient\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = httpclient.InferenceServerClient(url=\"localhost:8080\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs_ids = httpclient.InferInput(\"input_ids\", shape=(-1,1) , datatype=\"TYPE_INT64\",)\n",
    "attention_mask = httpclient.InferInput(\"attention_mask\", shape=(-1,1) , datatype=\"TYPE_INT64\",)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_to_translate = [\"I am learning how to use Triton Server for Machine Learning\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tokenizer(model_name):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    return tokenizer\n",
    "\n",
    "\n",
    "def tokenize_text(model_name, text):\n",
    "    tokenizer = get_tokenizer(model_name)\n",
    "    tokenized_text = tokenizer(text, padding=True, return_tensors=\"np\")\n",
    "    return tokenized_text.input_ids, tokenized_text.attention_mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_inference_input(model_name, text):\n",
    "    inputs = []\n",
    "    input_ids, attention_mask = tokenize_text(model_name, text)\n",
    "    inputs.append(httpclient.InferInput(\"input_ids\", input_ids.shape, \"INT64\"))\n",
    "    inputs.append(httpclient.InferInput(\"attention_mask\", attention_mask.shape, \"INT64\"))\n",
    "\n",
    "    inputs[0].set_data_from_numpy(input_ids.astype(np.int64), binary_data=False)\n",
    "    inputs[1].set_data_from_numpy(attention_mask.astype(np.int64), binary_data=False)\n",
    "    return inputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = generate_inference_input(MODEL_NAME, text_to_translate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tritonclient.http._infer_input.InferInput at 0x106c57a90>,\n",
       " <tritonclient.http._infer_input.InferInput at 0x15a6003d0>]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = httpclient.InferRequestedOutput(\"generated_indices\", binary_data=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is the code for the authentication part to bypass the login page\n",
    "import requests\n",
    "\n",
    "HOST = \"http://localhost:8080/\"\n",
    "USERNAME = \"user@example.com\"\n",
    "PASSWORD = \"12341234\"\n",
    "\n",
    "session = requests.Session()\n",
    "response = session.get(HOST)\n",
    "\n",
    "headers = {\n",
    "    \"Content-Type\": \"application/x-www-form-urlencoded\",\n",
    "}\n",
    "\n",
    "data = {\"login\": USERNAME, \"password\": PASSWORD}\n",
    "session.post(response.url, headers=headers, data=data)\n",
    "session_cookie = session.cookies.get_dict()[\"authservice_session\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "SESSION = session_cookie\n",
    "SERVICE_HOSTNAME = \"m2m100-translation-inference-service.default.example.com\"\n",
    "INGRESS_HOST = \"localhost\"\n",
    "INGRESS_PORT = \"8080\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Request was successful.\n",
      "{\"name\":\"m2m100_translation_model\",\"versions\":[\"1\"],\"platform\":\"python\",\"inputs\":[{\"name\":\"input_ids\",\"datatype\":\"INT64\",\"shape\":[-1,-1]},{\"name\":\"attention_mask\",\"datatype\":\"INT64\",\"shape\":[-1,-1]}],\"outputs\":[{\"name\":\"generated_indices\",\"datatype\":\"FP32\",\"shape\":[-1,-1]}]}\n"
     ]
    }
   ],
   "source": [
    "url = f\"http://{INGRESS_HOST}:{INGRESS_PORT}/v2/models/m2m100_translation_model\"\n",
    "\n",
    "# Define headers\n",
    "headers = {\n",
    "    \"Cookie\": f\"authservice_session={SESSION}\",\n",
    "    \"Host\": SERVICE_HOSTNAME,\n",
    "}\n",
    "\n",
    "# Make the HTTP request\n",
    "response = requests.get(url, headers=headers)\n",
    "\n",
    "# Check the response\n",
    "if response.status_code == 200:\n",
    "    print(\"Request was successful.\")\n",
    "    print(response.text)\n",
    "else:\n",
    "    print(f\"Request failed with status code {response.status_code}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'MTY5NTg0MzYyOHxOd3dBTkUwelMwUk5UMXBIVmpkRlNWbFVWbEl5UkROU1FsZE5SVVJYV2xGVVVETlVTMU15UkVaWldUTTBOMEZGTmtSQ1RVNUtTVUU9fJeO7m9NIqQhd7wSduSzaju2JS3xSK6kji2a_JHCZ6xf'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SESSION\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tritonclient.http._requested_output.InferRequestedOutput at 0x15b82aa90>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<HTTPSocketPoolResponse status=200 headers={'content-length': '266', 'content-type': 'application/json', 'date': 'Wed, 27 Sep 2023 19:40:44 GMT', 'x-envoy-upstream-service-time': '15149', 'server': 'istio-envoy'}>\n",
      "**********\n"
     ]
    }
   ],
   "source": [
    "results = client.infer(model_name=\"m2m100_translation_model\", inputs=inputs, outputs=[outputs], headers=headers)\n",
    "inference_output = results.as_numpy('generated_indices')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[     2, 128088,  71714,    720,  12089,    438,  85959,    102,\n",
       "         55728,  37578,  53140,    311,    103,   2447,     82,   2786,\n",
       "          3194,    720,  12089,    438,  28668,  21552,  55125,    360,\n",
       "             2]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inference_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded_output = tokenizer.batch_decode(inference_output, skip_special_tokens=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Ninajifunza jinsi ya kutumia Mtandao wa Triton ili Kujifunza Kutumia Mashine']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoded_output\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are able to infer using triton.\n",
    "\n",
    "The next step is to build the production pipeline that will scale on GCP."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Later we will update this code to use grpc because it have been proven to be faster and more efficient than http.\n",
    "\n",
    "One of the main advantages of gRPC over HTTP is that it is faster and more efficient. This is due to several factors:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### STart here tommorow night\n",
    "\n",
    "https://github.com/kserve/kserve/tree/master/docs/samples/multimodelserving/triton\n",
    "\n",
    "https://towardsdatascience.com/kserve-highly-scalable-machine-learning-deployment-with-kubernetes-aa7af0b71202\n",
    "\n",
    "\n",
    "$(kubectl get inferenceservices m2m100-translation-inference-service -o jsonpath='{.status.url}' | cut -d \"/\" -f 3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " docker run --rm -p 8000:8000 -p 8001:8001 -p 8002:8002 -v ${PWD}/triton_model_repository:/models nvcr.io/nvidia/tritonserver:23.06-py3 bash -c \"pip install transformers==4.30.2  sentencepiece==0.1.99 && tritonserver --model-repository=/models\"\n",
    "\n",
    "\n",
    " - https://medium.com/@fractal.ai/bloom-3b-optimization-deployment-using-triton-server-part-1-f809037fea40"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stop here because I failed  to load the model in the kubernetes cluster due to memory limitation issues\n",
    "\n",
    "[400] Failed to process the request(s) for model instance 'm2m100_translation_model_0', message: Stub process 'm2m100_translation_model_0' is not healthy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next step is to document the learning and learning about memory usage"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e7d420a2576d2f2cf4aee17bb1c719cb2b545f2d9fd7bdced2270e528bc643b9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
