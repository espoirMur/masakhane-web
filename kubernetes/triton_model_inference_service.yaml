apiVersion: "serving.kserve.io/v1beta1"
kind: "InferenceService"
metadata:
  name: "m2m100-translation-inference-service"
spec:
  predictor:
    volumes:
    - name: dshm
      emptyDir:
        medium: Memory
        sizeLimit: "2048Mi" # this is to solve the issue with the shared memory
    triton:
      image: espymur/triton-onnx:dev
      volumeMounts:
      - mountPath: /dev/shm
        name: dshm
        readOnly: false
      resources:
        requests:
          memory: "2Gi"
          cpu: "1"
      storageUri: "gs://masakhane-models/triton_model_repository"
